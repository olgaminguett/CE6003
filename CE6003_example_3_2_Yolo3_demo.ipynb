{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CE6003_example_3_2_Yolo3_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/CE6003/blob/master/CE6003_example_3_2_Yolo3_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3rj6mWPfkNV"
      },
      "source": [
        "\n",
        "# Object Detection with Yolo v3\n",
        "\n",
        "The [*You Only Look Once* detector (YOLO)](https://pjreddie.com/darknet/yolo/) is a real-time single stage object detection system based on CNNs. This demonstration notebook is based on the following implementation of Yolo in Keras - https://github.com/qqwweee/keras-yolo3. \n",
        "\n",
        "For many machine vision applications, an open source implementation may be useful to rapidly develop a vision complex system. However care must be taken in using or deploying a system, as failure to understand the underlying open source architecture(s) can lead to unexpected system limitations or failures. For example Yolo may not perform well when required to detect many small objects in a scene (can you explain why?).\n",
        "\n",
        "![YOLO v3](https://pjreddie.com/media/image/yologo_2.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymWTbTWUfq4G"
      },
      "source": [
        "!git clone https://github.com/qqwweee/keras-yolo3.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1evhE7aa4x7f"
      },
      "source": [
        "First, we'll load TensorFlow v1.x, as required by this YOLOv3 model.\n",
        "(Note: As Tensorflow 1.x is now the legacy version of tensorflow, do not spend time trying to understand tensorflow 1.x commands etc. It is better to study [examples](https://www.tensorflow.org/tutorials) written in Tensorflow 2.x as it more user friendly and integrated with Keras/python better)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lub8cyT4yR5"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEDr9IfXxsYY"
      },
      "source": [
        "*To* ensure this example runs as fast as possible, from the menu above select **Edit > Notebook settings or Runtime > Change runtime type** and select GPU as the Hardware accelerator option.\n",
        "\n",
        "Let's test that we are running using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjk-6QUKxtoD"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zUnwF97xvri"
      },
      "source": [
        "And now we can load Keras and the rest of the Python libraries we need into our notebook runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-FDxeq1gOvd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b97e8e-736d-4f2a-dff3-01e85b7e79a4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "\n",
        "ROOT_DIR = os.path.abspath(\"./keras-yolo3\")\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "from yolo3.utils import *\n",
        "from yolo3.model import *\n",
        "\n",
        "# And various libraries for image manipulation and plotting\n",
        "import random\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrL7D8ppjztF"
      },
      "source": [
        "## Converting pre-trained weights\n",
        "\n",
        "We can leverage some weights that are pre-trained from the the [YOLO website](https://pjreddie.com/darknet/yolo/). This version of YOLO is developed with an open source neural network framework called Darknet, so we need to convert the weights from the Darknet format into the HDF5 format that TensorFlow/Keras can read.\n",
        "\n",
        "Our pre-trained model is trained on the [Microsoft COCO](http://cocodataset.org/#home) dataset. ![COCO Examples](http://cocodataset.org/images/coco-examples.jpg)\n",
        "\n",
        "The COCO has several features:\n",
        "\n",
        "* Object segmentation;\n",
        "* Recognition in context;\n",
        "* Superpixel stuff segmentation;\n",
        "* 330K images (>200K labeled);\n",
        "* 1.5 million object instances;\n",
        "* 80 object categories;\n",
        "* 91 stuff categories;\n",
        "* 5 captions per imag\n",
        "* 250,000 people with keypoints.\n",
        "\n",
        "HDF5 (.h5, .hdf5) is a file format suitable for storing large multidimensional numeric arrays (e.g. models, data files). HDF stands for Hierarchical Data Format, and can store everything about your model, including:\n",
        "\n",
        "* The architecture of the model;\n",
        "* The weights of the model;\n",
        "* The training configuration (loss, optimizer);\n",
        "* The state of the optimizer, so you can resume training exactly where you left off.\n",
        "\n",
        "You can read more about the HDF5 file format [here](http://docs.h5py.org/en/stable/quick.html).\n",
        "\n",
        "The weights are about 240MB to download. The conversion will take a minute.\n",
        "\n",
        "**Note:** As the model we are leveraging uses Tensorflow 1.x, you can safely ignore some warnings about deprecated functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNHqOKZ7fd39"
      },
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights -O keras-yolo3/yolov3.weights\n",
        "!python keras-yolo3/convert.py keras-yolo3/yolov3.cfg keras-yolo3/yolov3.weights keras-yolo3/model_data/yolo.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfOXjxSqCi6F"
      },
      "source": [
        "Next, we need to setup a class_names array for our class detections, and a set of anchors.\n",
        "\n",
        "The YOLO model returns a class index to represent the class, but we need the array to convert from the index number into a human readable string. These are just the standard 80 classes from COCO.\n",
        "\n",
        "YOLO also uses anchor boxes - based on the intuition that most bounding boxes have common width to height ratios. Instead of predicting bounding boxes directly, YOLO works of a predetermined set of box sizes, called anchor boxes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS-eV4KUz4Cv"
      },
      "source": [
        "# We need to load our COCO class names\n",
        "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
        "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
        "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
        "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
        "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
        "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
        "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "               'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "# YOLO uses anchor boxes - setup our array of common width/heights\n",
        "anchors = np.array([[ 10,  13],\n",
        "       [ 16,  30],\n",
        "       [ 33,  23],\n",
        "       [ 30,  61],\n",
        "       [ 62,  45],\n",
        "       [ 59, 119],\n",
        "       [116,  90],\n",
        "       [156, 198],\n",
        "       [373, 326]])\n",
        "anchors = np.divide(anchors, 1.0) # convert our int array to float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGjtyHTYC9DZ"
      },
      "source": [
        "Next, we'll build our YOLO model.  This uses the TensorFlow 1.x *session*-style API semantics.  You don't need to worry too much about this, as the TF2.x Keras style API is much nicer.\n",
        "\n",
        "**Note:** As the model we are leveraging uses Tensorflow 1.x, you can safely ignore some warnings about deprecated functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvHZoudnfgCb"
      },
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "# Set the expected image size for the model - the model expects an image size \n",
        "# where both the width and height are multiples of 32\n",
        "model_image_size = (416, 416)\n",
        "assert model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
        "assert model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
        "\n",
        "# Create YOLO model\n",
        "model_path = os.path.join(ROOT_DIR, \"model_data/yolo.h5\")\n",
        "yolo_model = load_model(model_path, compile=False)\n",
        "\n",
        "# Disable warnings about deprecated TF 1.x functions vis-a-vis TF 2.x API\n",
        "try:\n",
        "    from tensorflow.python.util import module_wrapper as deprecation\n",
        "except ImportError:\n",
        "    from tensorflow.python.util import deprecation_wrapper as deprecation\n",
        "deprecation._PER_MODULE_WARNING_LIMIT = 0\n",
        "\n",
        "# Generate output tensor targets for bounding box predictions\n",
        "# \n",
        "# Predictions for individual objects are based on a probability score \n",
        "# threshold of 0.3, and an IoU threshold for non-max suppression of 0.45\n",
        "#\n",
        "# When run, this will evaluate the YOLO model on given inputs, and return\n",
        "# filtered bounding boxes to us\n",
        "\n",
        "input_image_shape = K.placeholder(shape=(2, ))\n",
        "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, num_classes, \n",
        "                                   input_image_shape, score_threshold=0.3,\n",
        "                                   iou_threshold=0.45)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbauX60grl7s"
      },
      "source": [
        "We'll create a helper function to detect objects in our image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bUF1HnFh30u"
      },
      "source": [
        "def detect_objects_in_image(image):\n",
        "    # we convert out image data to 32-bit float, normalise it (between 0.0 and 0.1)\n",
        "    image_data = np.array(image, dtype='float32')\n",
        "    image_data /= 255.\n",
        "    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "\n",
        "    # Predict classes and locations using Tensorflow session\n",
        "    sess = K.get_session()\n",
        "    out_boxes, out_scores, out_classes = sess.run(\n",
        "                [boxes, scores, classes],\n",
        "                feed_dict={\n",
        "                    yolo_model.input: image_data,\n",
        "                    input_image_shape: [image.size[1], image.size[0]],\n",
        "                    K.learning_phase(): 0\n",
        "                })\n",
        "    return out_boxes, out_scores, out_classes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPuEeeztsCss"
      },
      "source": [
        "And finally, a helper function to draw bounding boxes and labels on our original images and display them, so that we can see the results of our inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx_uAnacrkql"
      },
      "source": [
        "def draw_detected_objects_in_image(image, out_boxes, out_scores, out_classes):\n",
        "    # Set up some display formatting\n",
        "    cmap = plt.get_cmap('tab20b') # select a dark qualitative colormap for labels/bounding boxes\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "    # Plot the image\n",
        "    img = np.array(image)\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(1, figsize=(12,9))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Use a random color for each class\n",
        "    unique_labels = np.unique(out_classes)\n",
        "    n_cls_preds = len(unique_labels)\n",
        "    bbox_colors = random.sample(colors, n_cls_preds)\n",
        "\n",
        "    # process each instance of each class that was found\n",
        "    for instance, class_index in reversed(list(enumerate(out_classes))):\n",
        "        predicted_class = class_names[class_index]\n",
        "        bbox = out_boxes[instance]\n",
        "        score = out_scores[instance]\n",
        "\n",
        "\n",
        "        # Unpack the bounding box coordinates\n",
        "        (y1, x1, y2, x2) = bbox\n",
        "\n",
        "        # Set the box dimensions\n",
        "        box_h = (y2 - y1) \n",
        "        box_w = (x2 - x1)\n",
        "        \n",
        "        # Add a box with the color for this class\n",
        "        color = bbox_colors[int(np.where(unique_labels == class_index)[0])]\n",
        "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=4, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(bbox)\n",
        "        \n",
        "        # Format the label to be added to the image for this instance and add it\n",
        "        label = 'Class: {}, Score: {:.2f}'.format(predicted_class, score)\n",
        "        plt.text(x1, y1, s=label, color='white', verticalalignment='top',\n",
        "                bbox={'color': color, 'pad': 0})\n",
        "        \n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xth3HuYvyqx"
      },
      "source": [
        "Okay, we now have enough boilerplate code written to grab a number of test images, and run them through our model to see how we get on.\n",
        "\n",
        "For the sake of simplicity, we're cheating hugely here as we're just using sample images that are taken from the COCO dataset (so our model has already been exposed to them during training, and would be expected to do really well on them), but feel free to pick other image URLs and add them to the array below to run them through YOLO.\n",
        "\n",
        "What is so impressive about deep convolutional classifiers and detectors is that, once they are fed with sufficient training data, they are unfazed by occlusions (where part of the object is hidden or obscured from view) or pose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sm2x_YziQGY"
      },
      "source": [
        "def url_to_image(url):\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\ttemp_image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\ttemp_image = cv2.imdecode(temp_image, cv2.IMREAD_COLOR)\n",
        "\ttemp_image = cv2.cvtColor(temp_image, cv2.COLOR_BGR2RGB) # OpenCV defaults to BGR, but we need RGB here..\n",
        "\tpil_image = Image.fromarray(temp_image)\n",
        "\treturn pil_image\n",
        "\n",
        "image_urls = [\"https://farm9.staticflickr.com/8319/7885077674_6901d14828_z.jpg\", \n",
        "              \"https://farm8.staticflickr.com/7201/6879489797_fa772d8a69_z.jpg\", \n",
        "              \"https://farm4.staticflickr.com/3830/9381322299_698270f4d3_z.jpg\",\n",
        "              \"https://farm8.staticflickr.com/7331/9280720567_b684d5cccf_z.jpg\",\n",
        "              \"http://farm4.staticflickr.com/3175/2961808668_1d557e34a0_z.jpg\",\n",
        "              \"http://farm3.staticflickr.com/2441/3541164845_6fbea2e89f_z.jpg\",\n",
        "              \"http://farm8.staticflickr.com/7296/9019745657_c8776db96f_z.jpg\",\n",
        "              \"http://farm8.staticflickr.com/7108/6901315528_676d32186e_z.jpg\",\n",
        "              \"http://farm9.staticflickr.com/8101/8514154431_b8f1b57dc9_z.jpg\"\n",
        "              ]\n",
        "\n",
        "for image_url in image_urls:\n",
        "  print(\"Considering image\", image_url)\n",
        "  image = url_to_image(image_url)\n",
        "  \n",
        "  if (image.size != model_image_size):\n",
        "    print(\"\\tResizing (with letterbox) from \", image.size, \" to \", model_image_size)\n",
        "    image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
        "\n",
        "  out_boxes, out_scores, out_classes = detect_objects_in_image(image)\n",
        "  draw_detected_objects_in_image(image, out_boxes, out_scores, out_classes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}