{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Etivity_assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d9-nEXis_gaV"
      ],
      "authorship_tag": "ABX9TyOsAeuetR9MaTV5SyFPxQDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/CE6003/blob/master/Etivity_assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-nEXis_gaV"
      },
      "source": [
        "# Etivity Assignment 3: Object co-localisation\n",
        "In this assignment we will use the [Deep Descriptor Transforming (DDT) Technique](https://arxiv.org/pdf/1707.06397.pdf) to perform object co-localisation. The object localisation task involves finding the location of the primary object in an image (same object as image classification result). In the object co-localisation task the goal is to find the objects in images all of the same class. As detailed in the DDT paper, this can be achieved in a unsuperivsed manner from the feature map outputs of a pre-trained network. (In the image: Green bounding box is ground truth, Red boxes are obtained from DDT)\n",
        "![link text](https://github.com/tonyscan6003/CE6003/blob/master/images/etivity3_assignment_img.JPG?raw=true)\n",
        "\n",
        "This technique (like Class activiation mapping) demonstrates that spatial information is contained with Deep Neural Networks trained for classification. This information can be used to generate region proposals or for direct object detection. The  unsupervised technique proposed in this paper is useful as it avoids the necessity of having bounding box information to adapt the pre-trained network for localisation.\n",
        "\n",
        "In this Assignment you will perform object co-localisation using the stanford dogs dataset and the pre-trained VGG-16 network. This Jupyter notebook loads the stanford dogs dataset and also sets up the pre-trained Keras VGG-16 model so that the features of the convolutional layer are output. (Output layers of the model can be changed if required).  \n",
        "\n",
        "In section 4. of the notebook you will implement the DDT algorithm and demonstrate prediction of bounding boxes. (Ground truth bounding boxes are available for comparison with your prediction).\n",
        "\n",
        "You may find the following information useful:\n",
        "\n",
        "*   [Sklearn PCA Functions](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "* [Tensorflow resize](https://www.tensorflow.org/api_docs/python/tf/image/resize) (interpolation function)\n",
        "*   OpenCV Connected Components (PyImageSearch example [link text](https://www.pyimagesearch.com/2021/02/22/opencv-connected-component-labeling-and-analysis/)) Note that if you want to use OpenCv on the output tensors then it will be necessary to need to convert the tensor to a numpy array uisng `.numpy()` and also ensure that have the numpy array in uint8 format `.astype(np.uint8)`\n",
        "* If you wish to display any of the images from the dataset, please note that they have been processed prior to input to the network. In order to unprocess the image for display, please use: `helper.unprocess_image(img)`\n",
        "\n",
        "In order to obtain an exemplary grade you will be required to implement the DDT+ algorithm. This will involve use of additional feature map information to refine the position of the bounding box compared to the basic approach (as shown in the images below). You will implement the DDT+ algorithm on the [Pascale Visual Object Classes Challenge](https://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf) \"Sheep\" class. This is more challenging than the stanford dogs dataset, as objects appear at different scales. Note that the DDT+ approach also causes failures in bounding boxes for many images, you can comment on this in your notebook. \n",
        "\n",
        "Image: DDT+ Performance (Cyan: DDT+ bouding boxes, Red DDT: bounding boxes, Green: Ground Truth) \n",
        "![link text](https://github.com/tonyscan6003/CE6003/blob/master/images/etivity3b_assignment.JPG?raw=true)\n",
        "\n",
        "* Note: Completed list of VoC Classes 0-airplane, 1-bicycle, 2-bird, 3-boat, 4-bottle, 5-bus, 6-car, 7-cat, 8-chair, 9-cow, 10-dining table, 11-dog, 12-horse, 13-motorbike, 14-person, 15-potted plant, 16-sheep, 17-sofa, 18-train, 19-TV/monitor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_rx0kwv8pRA"
      },
      "source": [
        "# 1. HouseKeeping\n",
        " Clone Repository & Import Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXb5G3eUQe2"
      },
      "source": [
        "# Clone repository to gain access to helper.py\n",
        "!git clone https://github.com/tonyscan6003/CE6003.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uKx_naKbo9c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import CE6003.python.helper as helper\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZmPHMgRWq9M"
      },
      "source": [
        "# Global Variables\n",
        "HW_trg = helper.myList[0]      # Target Input Image size\n",
        "batch_size = helper.myList[1]  # Batch Size\n",
        "data_set = [\"stanford_dogs\"]   # Dataset (and add class integer to list for VoC))\n",
        "#data_set = [\"voc\",16]    # VoC Dataset with Sheep Class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTKCLOSsVkt"
      },
      "source": [
        "## 2A. Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWIHP-Ja9WPY"
      },
      "source": [
        "\n",
        "if data_set[0] == \"voc\":   \n",
        "   src_train_dataset,info = tfds.load(data_set[0],split='train',with_info=True)\n",
        "   src_val_dataset,info = tfds.load(data_set[0],split='validation',with_info=True)\n",
        "   src_test_dataset,info = tfds.load(data_set[0],split='test',with_info=True)\n",
        "elif data_set[0] == \"stanford_dogs\":\n",
        "   src_train_dataset,info = tfds.load(data_set[0],split='train',with_info=True)\n",
        "   src_test_dataset,info = tfds.load(data_set[0],split='test',with_info=True)\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndtoCeN3XfVC"
      },
      "source": [
        "## 2B. Create Train and Test dataset splits \n",
        "\n",
        "Note that the images from the stanford dog and many other datasets are not uniform in size. The `gen_datasets` function calls other routines from helper.py that scale the images from the dataset so the longest side fits into the 224 x 224 input window size of VGG16. The aspect ratio of the image is preserved, so the shorter side of the image is padded with zeros. The shorter side is randomly translated providing some data augmentation. (Augmentation is not important for this task). The routines also scale ground truth bounding boxes to match the scale and translation of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhuPmVI8_-op"
      },
      "source": [
        "train_dataset, test_dataset =helper.gen_datasets(data_set,src_train_dataset,src_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G75Q5CYU__ac"
      },
      "source": [
        "Display Some Training Images with Ground truth Bounding Box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib3p2gAAA9MS"
      },
      "source": [
        "\n",
        "helper.display_dataset_img(train_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w28N1Uv-NFg"
      },
      "source": [
        "## 3A. Import and Setup VGG model\n",
        "For this assignment we will use the convolutional layers of the VGG-16 module. The sturucture & layer names of the VGG-16 can be viewed on [Netscope](https://ethereon.github.io/netscope/#/preset/vgg-16). The layer_names variable allows selection of the output layer(s) of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9uiymlY-MGd"
      },
      "source": [
        "# Select Output Layers\n",
        "layer_names = ['block5_conv3']\n",
        "\n",
        "# Load base model\n",
        "def base_vgg_model():\n",
        "   IMG_SHAPE = (HW_trg, HW_trg, 3)\n",
        "   base_vgg_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
        "                                             include_top=False,\n",
        "                                             weights='imagenet')\n",
        "   \n",
        "   op_list=[base_vgg_model.get_layer(layer).output for layer in layer_names]\n",
        "   base_model= tf.keras.Model(inputs=base_vgg_model.input, outputs=op_list)\n",
        "\n",
        "   return base_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOLjyxvmGgEK"
      },
      "source": [
        "base_model = base_vgg_model()\n",
        "base_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-1FX4FuwxSn"
      },
      "source": [
        "## 3B. Extract output features maps from Model.\n",
        "\n",
        "The function `gen_batch_features` in the code cell below applies `n_img` images from the test or training set to the model. The function returns batch tensors of the image, ground truth boxes and features. The 0 dimension of the tensor corresponds to each image/feature pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FY6rDPdPIK0c"
      },
      "source": [
        "def gen_batch_features(train_dataset,n_img):\n",
        "\n",
        "    cntr = 0\n",
        "    for img, boxes, obj_cen, labels in train_dataset.take(n_img):\n",
        "       img = img.to_tensor(shape=[batch_size, HW_trg, HW_trg, 3])\n",
        "       A = base_model(img)\n",
        "\n",
        "       # Append output features \n",
        "       if cntr >0:\n",
        "          op_features = tf.concat([op_features,A],axis=0)\n",
        "          img_batch = tf.concat([img_batch,img],axis=0)\n",
        "          boxes_batch = tf.concat([boxes_batch,boxes],axis=0)\n",
        "       else:   \n",
        "          op_features= A\n",
        "          img_batch = img\n",
        "          boxes_batch = boxes\n",
        "       cntr +=1  \n",
        "     \n",
        "    return img_batch,boxes_batch,op_features   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxJAKzVqO4Zi"
      },
      "source": [
        "n_img=100 # Default value.\n",
        "img_batch,batch_boxes,op_features=gen_batch_features(train_dataset,n_img)\n",
        "img_batch_test,batch_boxes_test,op_features_test=gen_batch_features(test_dataset,n_img)\n",
        "print(np.shape(op_features))\n",
        "print(np.shape(batch_boxes_test))\n",
        "print(np.shape(img_batch_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nhwxol0bdRr"
      },
      "source": [
        "## 4. DDT Algorithm\n",
        "\n",
        "In the code cells below add your implementation of the DDT Algorithm for object co-localisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfiXFRX_P4wk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHkzgNgBrRyw"
      },
      "source": [
        "# 5. Display results\n",
        "\n",
        "You can display your results using the code cell below. You can display an image(s) similar to that shown at the start of the notebook, containing the image, ground truth bounding box and the bounding box produced by the DDT method. You can also show the binary map produced by indicator P (positive values).\n",
        "\n",
        "The function `helper.image_with_gt_boxes(img,boxes,colour)` can be used to plot a bounding box on an image where the bounding box is in the form [ymin,xmin,ymax,xmax] and values are normalised between zero and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVIoiJX9QiJX"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 5))\n",
        "n=8\n",
        "\n",
        "# Iterate through some images\n",
        "for k in range(n):\n",
        "   curr_img = np.asarray(helper.unprocess_image(img_batch_test[k, :, :,:]))\n",
        "   # draw a green rectangle to visualize the bounding rect\n",
        "   curr_img=helper.image_with_gt_boxes(curr_img,batch_boxes_test[k],(0,255,0))\n",
        "\n",
        "   # Plot image and show related indicator\n",
        "   ax = fig.add_subplot(1, n, k + 1, xticks=[], yticks=[])\n",
        "   ax.imshow(curr_img)\n",
        "   ax.set_title('Input Image')  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}