{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Example_5_2_CNN_Object_det.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGl7enVcvHYj3YnZcis4ci",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/CE6003/blob/master/Example_5_2_CNN_Object_det.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzYE64j7lTD_"
      },
      "source": [
        "#Example 5_2: Simple Object Detection in Video\n",
        "\n",
        "This example notebook demonstrates object detection on Youtube videos using Tensorflow Object detection API & openCV. This notebook can be used as a starting point for object detection & tracking based projects.\n",
        "![link text](https://github.com/tonyscan6003/CE6003/blob/master/images/plan_land_example_5_2.JPG?raw=true)\n",
        "\n",
        "\n",
        "Note that this notebook only demonstrates raw object detection with the Pre-trained Tensorflow object detection models on a video stream. [Object tracking](https://arxiv.org/abs/1907.12740) would also be required to be implemeted to ensure consistent identification of an object from frame to frame. (The notebook is configured to display detection of just one object type. See step 4 to change detected object type)\n",
        "\n",
        "In step 3 below the object detection model can be set from a selection of models available in Tensorflow object detection API including SSD and RCNN implementations.\n",
        "\n",
        "In step 4 the test video can be changed along with the target object, the number of frames processed and the starting frame in the video. (Note that 1 second of video may correspond to 30 or 60 frames depending on the source video frame rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy4JhdaUFXXU"
      },
      "source": [
        " Mount google drive to store output video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO1qV8RUQcIO"
      },
      "source": [
        "# Mount google drive\n",
        "#https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory?rq=1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfG7bA8dqbIR"
      },
      "source": [
        "#Step 1:\n",
        "Obtain access to youtube video for object detection. Using [Pafy](https://pypi.org/project/pafy/) a Python library to download YouTube content and retrieve metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Wbq3o8sgjM"
      },
      "source": [
        "!pip install pafy\n",
        "!pip install --upgrade youtube_dl\n",
        "import cv2, pafy\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5O4udUYobjS"
      },
      "source": [
        "#Step 2: \n",
        "Setup [Tensorflow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjICbMEfgisE"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "from tqdm import tqdm  \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "print(tf.__version__)\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phtkOiSNofZi"
      },
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WsTox5MojrB"
      },
      "source": [
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br_0MaKfoqnb"
      },
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huZIFNeqs3kF"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "ALL_MODELS = {\n",
        "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
        "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
        "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
        "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
        "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
        "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
        "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
        "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
        "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-SSpaQwsTz5"
      },
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFxj7UhwGVWq"
      },
      "source": [
        "#Step 3: Select Model\n",
        "In the code cell below, the object detection model can be selected from the list above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vhpET0FtEzs"
      },
      "source": [
        "model_display_name = 'CenterNet HourGlass104 512x512'\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFHyG-aRtRaA"
      },
      "source": [
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd7YfvqNGioS"
      },
      "source": [
        "#Step 4\n",
        "Run model and process video (frame by frame), write an output video with bounding box added. (Note that more information including class is available from the object detector and can be annotated to the video)\n",
        "\n",
        "Note that the object to be detected in the video must be selected from the [MS COCO labels](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/). Set the `obj_label` to the correct label value from the list to identify the object of interest.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o42AxtMsaQUM"
      },
      "source": [
        "# Configuration\n",
        "url = \"https://www.youtube.com/watch?v=3FXUw98rrUY\"   # Recognising pedestrians (set obj_label =1 for Person)\n",
        "#url = \"https://www.youtube.com/watch?v=Psw6uL8x8Ak\"     # Ariport take off (set obj_label =5 for Aiplane)\n",
        "\n",
        "no_frames = 250         # Set total number of frames\n",
        "strt_frame = 300         # Set starting frame\n",
        "obj_label = 1           # Set object to be detected (MS COCO label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_YIcPUwzTOZ"
      },
      "source": [
        "\n",
        "# Helper function to Add bounding box to current frame \n",
        "# Applies this to single object in frame\n",
        "greenColor = (0, 255, 0)\n",
        "lineThickness = 2\n",
        "\n",
        "def box_2_pixel(bb,frame):\n",
        "    \n",
        "    bb = results['detection_boxes'][0]\n",
        "    score = results['detection_scores'][0]\n",
        "    obj_class = tf.cast((results['detection_classes'][0]),tf.int32)\n",
        "    x_pix = np.shape(frame)[0]\n",
        "    y_pix = np.shape(frame)[1]\n",
        "    ptr = 0\n",
        "    # Add bounding box to image frames.\n",
        "    for boxes in bb:\n",
        "       # Format convert to rectangular from Corner\n",
        "       y_TL = tf.cast((x_pix*(boxes[0])),tf.uint16)\n",
        "       x_TL = tf.cast((y_pix*(boxes[1])),tf.uint16)\n",
        "       y_BR = tf.cast((x_pix*(boxes[2])),tf.uint16)\n",
        "       x_BR = tf.cast((y_pix*(boxes[3])),tf.uint16) \n",
        "       bb_cen = (tf.cast((y_pix*(boxes[1]+boxes[3])/2),tf.uint16),tf.cast((x_pix*(boxes[0]+boxes[2])/2),tf.uint16))\n",
        "       # Add \n",
        "       if score[ptr]>0.5 and obj_class[ptr]==obj_label:\n",
        "          # Add bounding box rectangle to current frame\n",
        "          cv2.rectangle(frame, (int(x_TL), int(y_TL)), (int(x_BR),int(y_BR)), greenColor, lineThickness)\n",
        "       ptr+=1   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfoDYs0RlLFq"
      },
      "source": [
        "# Use Pafy to read video frames, apply tensorflow model and write to video\n",
        "# using OpenCV writen functions. \n",
        "writer = None\n",
        "video = pafy.new(url)\n",
        "frame_no =0             # Frame Counter Intialised to 0\n",
        "print(video.title)\n",
        "streams = video.streams\n",
        "\n",
        "for s in streams:\n",
        "    print(s.resolution, s.extension, s.get_filesize(), s.url)\n",
        "best  = video.getbest(preftype=\"mp4\")\n",
        "capture = cv2.VideoCapture(best.url)\n",
        "capture.set(1,strt_frame) \n",
        "\n",
        "for vals in tqdm(range(no_frames)):\n",
        "  # Read in frames\n",
        "  check, frame = capture.read()\n",
        "\n",
        "  if frame is None:\n",
        "    break\n",
        "\n",
        "  # Process with tracker/object detector.\n",
        "  results = hub_model(np.expand_dims(np.asarray(frame),axis=0))\n",
        "  frame_no+=1\n",
        "\n",
        "  # draw the final bounding boxes\n",
        "  box_2_pixel(results,frame)\n",
        "\n",
        "  # Build a frame of our output video\n",
        "  if writer is None:\n",
        "    # Initialize our video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP80') #codec\n",
        "    writer = cv2.VideoWriter('video.webm', fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "  # Write to video\n",
        "  writer.write(frame)\n",
        "\n",
        "writer.release()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTxs1KzlF4xg"
      },
      "source": [
        "!cp video.webm /content/gdrive/MyDrive/video.webm\n",
        "#!ls /content/gdrive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELPqXSgeZoqE"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "webm = open('video.webm','rb').read()\n",
        "data_url = \"data:video/webm;base64,\" + b64encode(webm).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/webm\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}